{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlace al gib de ejmplo : https://github.com/dylanjcastillo/random/tree/main/self-organizing-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering Documents\n",
    "You should think of the clustering process in three steps:\n",
    "\n",
    "Generate numerical vector representations of documents using OpenAI’s embedding capabilities.\n",
    "Apply a clustering algorithm on the vectors to group the documents.\n",
    "Generate a title for each cluster summarizing the articles contained in it.\n",
    "That’s it! Now, you’ll see how that looks in practice.\n",
    "\n",
    "Import the Required Packages\n",
    "Start by importing the required Python libraries. Copy the following code in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hdbscan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/10.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.5/10.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 16.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 45.3 MB/s eta 0:00:00\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: safetensors, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.17.0 huggingface-hub-0.29.1 safetensors-0.5.3 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.14.1-cp312-cp312-win_amd64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n",
      "Using cached tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Using cached tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.3/4.3 MB 51.0 MB/s eta 0:00:00\n",
      "Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 57.3 MB/s eta 0:00:00\n",
      "Using cached keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp312-cp312-win_amd64.whl (306 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.13.0 keras-3.8.0 libclang-18.1.1 markdown-3.7 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.1 protobuf-5.29.3 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from sumy) (2.32.3)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nltk>=3.0.2 (from sumy)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting chardet (from breadability>=0.1.20->sumy)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting lxml>=2.0 (from breadability>=0.1.20->sumy)\n",
      "  Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests>=2.7.0->sumy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from requests>=2.7.0->sumy) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n",
      "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.9 MB/s eta 0:00:00\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.3/6.3 MB 29.9 MB/s eta 0:00:00\n",
      "Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.8/3.8 MB 32.6 MB/s eta 0:00:00\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: breadability, docopt\n",
      "  Building wheel for breadability (setup.py): started\n",
      "  Building wheel for breadability (setup.py): finished with status 'done'\n",
      "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21739 sha256=166b0fdbd800d509de41a89891a1126cc1273af0f69e1926f89405e2b9077efd\n",
      "  Stored in directory: c:\\users\\extas\\appdata\\local\\pip\\cache\\wheels\\32\\99\\64\\59305409cacd03aa03e7bddf31a9db34b1fa7033bd41972662\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13775 sha256=98c8e623e53d979df81036e8058d910796b14a5fd8b3331de6d6462533815548\n",
      "  Stored in directory: c:\\users\\extas\\appdata\\local\\pip\\cache\\wheels\\1a\\bf\\a1\\4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built breadability docopt\n",
      "Installing collected packages: docopt, pycountry, lxml, chardet, nltk, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 chardet-5.2.0 docopt-0.6.2 lxml-5.3.1 nltk-3.9.1 pycountry-24.6.1 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install sumy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from torch) (72.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\extas\\anaconda3\\envs\\nuclio\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.5/204.1 MB 65.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 19.9/204.1 MB 59.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 31.7/204.1 MB 53.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 45.1/204.1 MB 55.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 59.0/204.1 MB 56.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 73.1/204.1 MB 58.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 87.6/204.1 MB 59.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 99.4/204.1 MB 59.3 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 113.5/204.1 MB 60.4 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 126.6/204.1 MB 60.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 140.5/204.1 MB 60.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 151.5/204.1 MB 59.7 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 165.2/204.1 MB 60.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 178.5/204.1 MB 60.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 192.4/204.1 MB 60.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 60.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 60.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 53.9 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 47.4 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 17.1 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, torch\n",
      "Successfully installed mpmath-1.3.0 sympy-1.13.1 torch-2.6.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from umap import UMAP\n",
    "import langid\n",
    "import tiktoken\n",
    "from transformers import pipeline\n",
    "import tensorflow\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "archivo .env\n",
    "API_KEY=tu_api_key_secreta\n",
    "DATABASE_URL=mysql://usuario:contraseña@localhost/db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API_KEY: None\n",
      "API_KEY: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carga las variables desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Accede a las variables\n",
    "api_key = os.getenv(\"API_KEY_OPENAI\")\n",
    "api_key1 = os.getenv(\"API_KEY_CHATGPT\")\n",
    "#db_url = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "print(f\"API_KEY: {api_key}\")\n",
    "print(f\"API_KEY: {api_key1}\")\n",
    "#print(f\"DATABASE_URL: {db_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('./data/pisosCl_CharGPT.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "descripcion_larga\n",
       "NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1915\n",
       "Leer todo no encontrado                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        51\n",
       "REDPISO VILLAVERDE BAJO - SAN CRISTOBAL inscrito en el RAIN Nº 000517/20.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       9\n",
       " ¡OBRAS INICIADAS! Descubre la comodidad y el estilo de vida moderno en nuestro  innovador residencial con piscina  que ofrece  viviendas con terrazas  de 2 y 3 dormitorios y 2 baños, áticos y bajos con jardí­n con unas vistas espectaculares de Madrid. ¿Conoces la climatización por suelo radiante/refrescante? En Durazno Residencial podrás disfrutar de las sensaciones que da pisar un suelo calentito en invierno y fresco en verano, acomodando tu casa para que tengas el confort que buscas en tu hogar. En Durazno Residencial encontrarás un residencial de Pisos en Carabanchel donde podrás  acondicionar la vivienda a tu personalidad , pudiendo elegir los acabados que mejor se adapten a ti y a tu estilo de vida, además tendrás cocinas completas con muebles de gran capacidad y diseños diferentes para que encuentres la combinación perfecta que refleje tu personalidad. Pensando en la sostenibilidad y la eficiencia energética, nuestro edificio cuenta con  paneles solares  que minimizan el consumo de energía para el suministro de agua caliente, proporcionándote un hogar más ecológico. Durazno Residencial es un edificio pensado para gente joven como tú, en el que podrás disfrutar de nuestras modernas instalaciones, incluyendo una  zona común con piscina  para refrescarte en los dí­as soleados y disfrutar con tu gente o descansar del bullicio de la capital. Cada vivienda en Durazno Residencial viene con trastero, y si lo deseas, la opción de garaje con preinstalación para cargador eléctrico en cada plaza. ¿Buscas  obra nueva en Carabanchel ? ¿Te interesa encontrar pisos nuevos en Carabanchel? Descubre la calidad, el confort y la innovación en Durazno Residencial, cerca del centro de salud de Puerta Bonita. Te invitamos a ser parte de esta experiencia única y moderna. ¡No pierdas la oportunidad de asegurar tu lugar en este emocionante proyecto! Contáctanos ahora para obtener más información y dar el primer paso hacia tu nuevo hogar en Durazno Residencial. ¡Bienvenido a tu vida mejorada en Carabanchel!                                                                                                                           6\n",
       "Magnífico Chalet en Albolote, situado en la prestigiosa zona de Cortijo del Aire. Esta impresionante vivienda ofrece una superficie construida de 219.00 m en una amplia parcela de 352.36 m. Con una cocina espaciosa de 9.47 m. Amplio y luminoso salón de 26.44 m con salida a una terraza de 20.10m. Este hogar cuenta con 2 dormitorios dobles y 2 sencillas, 2 baños completos y 1 aseo. Dormitorio principal con baño en suite y además con un ventanal con preciosas vistas a la Sierra. Patio jardín de 197m con piscina privada. Dos plantas mas sótano. Listo para ser habitado, este chalet se entrega con una cocina equipada de alta calidad. Los detalles interiores incluyen elegante carpintería de madera, suelos de cerámica impecables y ventanas de aluminio/climalit que ofrecen una orientación sureste. Entre sus lujosas comodidades, se destacan 2 estufas de pellets de alto rendimiento ( de marca Sola), aire acondicionado con bomba de frío/calor en todas las habitaciones y en el salón, armarios empotrados, baño en suite, calefacción, despensa, garaje doble, exuberante jardín, línea telefónica, piscina privada con agua salina, puerta blindada, sistemas automáticos en las puertas de garaje, sótano, techos altos, amplia terraza, videoportero y más. Situado en una urbanización exclusiva, esta propiedad ofrece vistas panorámicas, entorno rodeado de naturaleza, proximidad a parques y zonas infantiles, además de acceso conveniente a servicios esenciales y transporte público. No pierdas la oportunidad de adquirir este espectacular chalet y disfrutar de la vida de lujo y comodidad que ofrece! Tu puedes ser el novio de esta fantástica vivienda, vamos a verlo? NOTA: El PVP indicado no incluye impuestos ni gastos de Escritura. Honorarios agencia (2,5% del precio de venta + IVA) no incluidos. Las superficies expresadas en esta página tienen carácter descriptivo y son aproximadas. Los precios pueden ser susceptibles de modificación sin previo aviso. *Esta vivienda se vende con la cocina amueblada y equipada con los electrodomésticos, y los baños tal como se muestran en las fotos, todos los demás muebles no incluyen en el precio.       6\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ... \n",
       "MASTERPISO vende piso, bajo exterior en la Calle Camarena Aluche Madrid, 62m2 construidos, 1 habitación, cocina amueblada integrada al salón, con electrodomésticos, horno, campana y vitrocerámica, 1 baño con plato de ducha, aparcamiento en superficie. Vivienda a estrenar, suelo tarima, calefacción bomba frio calor, y agua caliente por termo eléctrico, ventanas, de PVC. , climalit, puente térmico y oscilo batientes, puertas lacadas blancas. Vivienda situada en la mejor zona de Aluche, zona muy tranquila y ajardinada, arteria principal de Aluche, con todos los servicios cerca, colegios, centro de salud, comercios, centros comerciales, supermercados, centros deportivos. A pocos minutos del intercambiador y Renfe de Aluche, Varias líneas de Autobuses 31, 121, 138. CAMBIO DE USO EN TRAMITE. Todos los gastos de Registro de cambio de uso, ANTES o DESPUÉS de escriturar, corren por cuenta del vendedor. El cambio de uso a vivienda ya está aprobado a instancia previa, por lo que se podría hacer uso de vivienda. CON POSIBILIDAD DE AMUEBLAR consultar precio. Para más información puede contactar con nosotros. Ven a visitarlo. Si quiere cambiar de vivienda, si quiere vender su inmueble, vender para compra o quiere saber el precio de mercado, hacemos valoraciones totalmente gratuitas y sin compromiso. VALORACIÓN GRATUITA ONLINE EN 2 MINUTOS EN NUESTRA PAGINA WEB www.agenciamasterpiso.es/valoramos-tu-casa/ Asesoramiento financiero gratuito a nuestros Clientes. El presente anuncio es a título informativo, no siendo vinculante, ya que puede contener errores, no siendo por tanto contractual en ningún caso.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1\n",
       "De Rojas servicios inmobiliarios pone a su disposición inmueble ubicado en pleno corazòn de Madrid.La vivienda exterior luminosa y situada en una 4ª planta consta de 191 m² distribuidos en 5 dormitorios con armarios empotrados 2 de ellos exteriores y 3 de ellos interiores pero luminosos por su ubicación, 3 cuartos de baños completos, 2 amplios salones con terraza y cocina.Situada en finca representativa y a reformar cuenta con ascensor en la finca, suelos y carpintería de madera originales, calefacción central con medidores individualizados, una vivienda por planta, excelente insonorización.De una ubicación excelente muy proximo a Plaza de España, vistas al Palacio Real y Templo de Deboh cuenta con una gran oferta gastronómica cultural y de ocio y gran conexión de transporte público.No dude en concertar una cita con nosotros.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
       "Homely One pone a la venta fantástica vivienda con Trastero y Plaza de Garaje enclavada en el demandado Barrio de Salamanca - Guindalera, finca con portero físico y 2 ascensores. La vivienda está en una 6ª planta exterior con varios huecos a la calle, siendo uno de ellos una magnifica terraza cerrada que nos permite crear un pequeño jardín hacia la fachada principal con amplias vistas al exterior debido a su gran altura.El inmueble tiene unas dimensiones de 198,00m2, con una distribución de amplias y confortables estancias. Accedemos a la vivienda mediante un amplio Hall recibidor que nos da paso a un enorme y luminoso Salón Comedor exterior con Terraza cerrada y orientación Este, lo que permite la entrada de gran cantidad de luz natural. En la distribución actual contamos con 5 Dormitorios, siendo 3 de ellos totalmente exteriores y 2 Baños completos. La Cocina abierta hacia el comedor, dispone de buena capacidad de almacenaje y cuenta con una pequeña Isla.  La vivienda se encuentra en buen estado de conservación, no obstante Homely One es un estudio de Arquitectura donde nuestro equipo de técnicos y decoradores nos plantean diferentes alternativas de distribución, en caso de querer reformar, donde podremos adaptar la vivienda a sus necesidades específicas. La vivienda tiene Calefacción Central.No deje pasar esta oportunidad de conocer la que pudiera ser su futura vivienda. En Homely One estaremos encantados de guiarle en la visita y darle un asesoramiento global. Solicita una visita a nuestros asesores Te encantará! NOTA: Las infografías corresponden con el estudio realizado por nuestro Equipo de Arquitectos. Reforma no incluida en el precio, Consultar.                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1\n",
       "Remax Activo Residencial ofrece este fantástico y maravilloso piso completamente reformado, en una quinta planta lleno de luz y claridad con vistas despejadas, en frente del intercomunicador de Plaza Castilla, perfectamente comunicado , trastero y una plaza de garaje.El edificio tiene ocho plantas y hablamos de la vivienda sito en la quinta planta, con zonas comunes, y plazas de garaje rotativas para los vecinos. Edificio de 1965. Calefacción central , portero y gas natural.Ofrecemos un piso con tres habitaciones dobles, una sencilla que actualmente hace funcion de cuarto blanco de lavanderia y planchado, o de estudio y dos baños completos.Una amplia cocina con ventana propia a un patio de manzana con mucha luz equipada con todos los electrodomesticos de alta gama, con espacio para una mesa y poder disfrutar de las comidas en familia y de los ratos con amigos mientras se cocina.Todo esto se encuentra en la parte izda de la casa despues de un amplio recibidor totalmente revestido con armarios empotrados.A la derecha se encuentra el amplio salon de 25.25 mt 2 con una gran luminosidad y grandes ventanales y una zona comedor de 16.21 mt 2 igualmente con amplios ventanales.Toda la vivienda con orientacion este-oeste, goza de gran cantidad de luz natural y Aire acondicionado por conductos. El piso tiene una situación muy  estratégica al encontrarse en el nudo norte del paseo de la Castellana dentro del plan estrategico norte de Madrid, cerca del nuevo centro de negocios de la capital , enfrente del intercomunicador de transporte de Pza Castilla, y de los juzgados.No dude en consultarme para cualquier informacion adiccional del inmueble al telf.: 630096680 o a la oficina Remax Activo Residencial que gustosamente le atenderemos y le resolveremos cualquier necesidad  inmobiliaria: viviendas, oficinas, locales comerciales, solares, fincas rústicas, terrenos para construir.. .                                                                                                                                                                                                                                                      1\n",
       "Descripción: Precioso piso reformado de 60m2 enfrente del Retiro, súper luminoso, todas las estancias con ventana exterior.~~Piso de 3 habitaciones, 1 baño con plato de ducha, salón, cocina totalmente equipada, interior con muchísima luz.~~Techos altos, suelos de tarima, ventanas blancas de climalit oscilobatientes, puertas lacadas en blanco, aire acondicionado en el salón.~~Ascensor, calefacción central, agua caliente por caldera de gas natural.~~Ubicación excelente en frente del Parque del Retiro.~~No dudes en venir a visitarlo!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       "Name: count, Length: 7842, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"descripcion_larga\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10041, 2)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar valores de texto 'NaN' con np.nan\n",
    "df = df.replace('NaN', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10041"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"descripcion_larga\"].isnull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1915 nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas donde 'descripcion_larga' es NaN\n",
    "df = df.dropna(subset=['descripcion_larga'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8126, 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tokens en la columna 'descripcion_larga': 2973215\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inicializa el codificador de tokens para el modelo que estés utilizando (ejemplo: GPT-3, GPT-4)\n",
    "# Para GPT-3 o GPT-4 puedes usar el codificador de 'gpt-3.5-turbo' o 'gpt-4'\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")  # Usa \"cl100k_base\" para GPT-3.5 y GPT-4\n",
    "\n",
    "# Función para contar los tokens de un texto\n",
    "def contar_tokens(texto):\n",
    "    return len(encoder.encode(texto))\n",
    "\n",
    "# Aplicar la función a cada fila de la columna 'descripcion'\n",
    "df['num_tokens'] = df['descripcion_larga'].apply(contar_tokens)\n",
    "\n",
    "# Ver el total de tokens\n",
    "total_tokens = df['num_tokens'].sum()\n",
    "\n",
    "print(f\"Total de tokens en la columna 'descripcion_larga': {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hay algunos anuncios con el texto en español y en ingles, utilizo langid para eliminar el texto en ingles\n",
    "df[\"descripcion_larga\"] = df[\"descripcion_larga\"].apply(lambda x: \"\" if langid.classify(x)[0] == \"en\" else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tokens en la columna 'descripcion_larga': 2965262\n"
     ]
    }
   ],
   "source": [
    "# Aplicar la función a cada fila de la columna 'descripcion'\n",
    "df['num_tokens'] = df['descripcion_larga'].apply(contar_tokens)\n",
    "\n",
    "# Ver el total de tokens\n",
    "total_tokens = df['num_tokens'].sum()\n",
    "\n",
    "print(f\"Total de tokens en la columna 'descripcion_larga': {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cargar el modelo de resumen preentrenado\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large-cnn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Función para resumir el texto\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresumir\u001b[39m(texto):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Resumir el texto usando el pipeline\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\transformers\\pipelines\\base.py:241\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    247\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "# Cargar el modelo de resumen preentrenado\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Función para resumir el texto\n",
    "def resumir(texto):\n",
    "    # Resumir el texto usando el pipeline\n",
    "    summary = summarizer(texto, min_length=30, max_length=60)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva columna con los resúmenes\n",
    "df['resumen'] = df['dscripcion_larga'].apply(resumir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###############COLAB\n",
    "\n",
    "# Cargar el modelo de resumen\n",
    "resumen_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Crear una función para resumir una fila de texto\n",
    "def resumir_texto(texto, max_length=150, min_length=50):\n",
    "    resumen = resumen_pipeline(texto, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return resumen[0]['summary_text']\n",
    "\n",
    "# Aplicar la función de resumen a toda la columna 'descripcion_larga'\n",
    "df['resumen'] = df['descripcion_larga'].apply(resumir_texto)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\extas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "NLTK tokenizers are missing or the language is not supported.\nDownload them by following command: python -c \"import nltk; nltk.download('punkt')\"\nOriginal error was:\n\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/spanish/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\extas/nltk_data'\n    - 'c:\\\\Users\\\\extas\\\\anaconda3\\\\envs\\\\Nuclio\\\\nltk_data'\n    - 'c:\\\\Users\\\\extas\\\\anaconda3\\\\envs\\\\Nuclio\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\extas\\\\anaconda3\\\\envs\\\\Nuclio\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\extas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\sumy\\nlp\\tokenizers.py:172\u001b[0m, in \u001b[0;36mTokenizer._get_sentence_tokenizer\u001b[1;34m(self, language)\u001b[0m\n\u001b[0;32m    171\u001b[0m     path \u001b[38;5;241m=\u001b[39m to_string(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m to_string(language)\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mLookupError\u001b[39;00m, zipfile\u001b[38;5;241m.\u001b[39mBadZipfile) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\nltk\\data.py:823\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mswitch_punkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunkers/maxent_ne_chunker\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\nltk\\data.py:678\u001b[0m, in \u001b[0;36mswitch_punkt\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PunktTokenizer \u001b[38;5;28;01mas\u001b[39;00m tok\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtok\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1743\u001b[0m PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/spanish/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\extas/nltk_data'\n    - 'c:\\\\Users\\\\extas\\\\anaconda3\\\\envs\\\\Nuclio\\\\nltk_data'\n    - 'c:\\\\Users\\\\extas\\\\anaconda3\\\\envs\\\\Nuclio\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\extas\\\\anaconda3\\\\envs\\\\Nuclio\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\extas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     17\u001b[0m     texto \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescripcion_larga\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Extrae el texto de la columna 'descripcion_larga'\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     resumen \u001b[38;5;241m=\u001b[39m \u001b[43mresumir_texto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Genera el resumen usando Sumy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     resumenes_sumy\u001b[38;5;241m.\u001b[39mappend(resumen)  \u001b[38;5;66;03m# Almacena el resumen\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Ahora 'resumenes_sumy' tiene los resúmenes de todas las filas\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[116], line 7\u001b[0m, in \u001b[0;36mresumir_texto\u001b[1;34m(texto, num_frases)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresumir_texto\u001b[39m(texto, num_frases\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     parser \u001b[38;5;241m=\u001b[39m PlaintextParser\u001b[38;5;241m.\u001b[39mfrom_string(texto, \u001b[43mTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspanish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m     summarizer \u001b[38;5;241m=\u001b[39m LsaSummarizer()\n\u001b[0;32m      9\u001b[0m     resumen \u001b[38;5;241m=\u001b[39m summarizer(parser\u001b[38;5;241m.\u001b[39mdocument, num_frases)\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\sumy\\nlp\\tokenizers.py:160\u001b[0m, in \u001b[0;36mTokenizer.__init__\u001b[1;34m(self, language)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_language \u001b[38;5;241m=\u001b[39m language\n\u001b[0;32m    159\u001b[0m tokenizer_language \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLANGUAGE_ALIASES\u001b[38;5;241m.\u001b[39mget(language, language)\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentence_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_sentence_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_language\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_word_tokenizer(tokenizer_language)\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\sumy\\nlp\\tokenizers.py:174\u001b[0m, in \u001b[0;36mTokenizer._get_sentence_tokenizer\u001b[1;34m(self, language)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mLookupError\u001b[39;00m, zipfile\u001b[38;5;241m.\u001b[39mBadZipfile) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNLTK tokenizers are missing or the language is not supported.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Download them by following command: python -c \"import nltk; nltk.download('punkt')\"\\n\"\"\"\u001b[39;00m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error was:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m    178\u001b[0m     )\n",
      "\u001b[1;31mLookupError\u001b[0m: NLTK tokenizers are missing or the language is not supported.\nDownload them by following command: python -c \"import nltk; nltk.download('punkt')\"\nOriginal error was:\n\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/spanish/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\extas/nltk_data'\n    - 'c:\\\\Users\\\\extas\\\\anaconda3\\\\envs\\\\Nuclio\\\\nltk_data'\n    - 'c:\\\\Users\\\\extas\\\\anaconda3\\\\envs\\\\Nuclio\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\extas\\\\anaconda3\\\\envs\\\\Nuclio\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\extas\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "# Definir la función de resumen\n",
    "def resumir_texto(texto, num_frases=5):\n",
    "    parser = PlaintextParser.from_string(texto, Tokenizer(\"spanish\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "    resumen = summarizer(parser.document, num_frases)\n",
    "    return \" \".join([str(sentence) for sentence in resumen])\n",
    "\n",
    "# Crear una lista para almacenar los resúmenes\n",
    "resumenes_sumy = []\n",
    "\n",
    "# Itera sobre todas las filas del DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    texto = row[\"descripcion_larga\"]  # Extrae el texto de la columna 'descripcion_larga'\n",
    "    resumen = resumir_texto(texto, num_frases=5)  # Genera el resumen usando Sumy\n",
    "    resumenes_sumy.append(resumen)  # Almacena el resumen\n",
    "\n",
    "# Ahora 'resumenes_sumy' tiene los resúmenes de todas las filas\n",
    "df[\"resumen_sumy\"] = resumenes_sumy  # Agrega los resúmenes al DataFrame en una nueva columna\n",
    "\n",
    "# Opcional: Imprimir los primeros resúmenes\n",
    "print(df[\"resumen_sumy\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la función a cada fila de la columna 'descripcion'\n",
    "df['num_tokens'] = df['resumen_sumy'].apply(contar_tokens)\n",
    "\n",
    "# Ver el total de tokens\n",
    "total_tokens = df['num_tokens'].sum()\n",
    "\n",
    "print(f\"Total de tokens en la columna 'resumen_sumy': {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutar en COLAB\n",
    "from transformers import pipeline\n",
    "\n",
    "# Inicializa el modelo de resumen\n",
    "resumen_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Crea una lista para almacenar los resúmenes\n",
    "resumenes = []\n",
    "\n",
    "# Itera sobre todas las filas del DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    texto = row[\"descripcion_larga\"]  # Extrae el texto de la columna 'descripcion'\n",
    "    resumen = resumen_pipeline(texto, max_length=150, min_length=50, do_sample=False)  # Genera el resumen\n",
    "    resumenes.append(resumen[0]['summary_text'])  # Almacena el resumen\n",
    "\n",
    "# Ahora 'resumenes' tiene los resúmenes de todas las filas\n",
    "df[\"resumen\"] = resumenes  # Agrega los resúmenes al DataFrame en una nueva columna\n",
    "\n",
    "# Opcional: Imprimir los primeros resúmenes\n",
    "print(df[\"resumen\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la función a cada fila de la columna 'descripcion'\n",
    "df['num_tokens'] = df['resumen'].apply(contar_tokens)\n",
    "\n",
    "# Ver el total de tokens\n",
    "total_tokens = df['num_tokens'].sum()\n",
    "\n",
    "print(f\"Total de tokens en la columna 'resumen': {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver filas con descripcion_larga duplicada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'descripcion_larga'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La fila más larga es la número 7310 con Espectacular piso en la zona centro totalmente reformado con proyecto de interiorismo a medida en primeras calidades. 3 dormitorios, un estudio y dos baños (un dormitorio con baño en suite) y un amplio y luminoso salón comedor.\n",
      " \n",
      " ***La vivienda*** Un elegante y espacioso hall te da la bienvenida a tu vivienda, siendo el perfecto espacio entre el bullicio del centro de Madrid y la paz de tu hogar. Desde el primer momento vislumbramos el amplio salón comedor tras un elegante espacio de reposo. \n",
      " \n",
      " Accedemos al comedor, donde una magnífica mesa ideal para comidas en familia y con amigos nos da la bienvenida. Al fondo, la zona de estar en la que los grandes balcones hacia la calle Salud son los protagonistas. El mobiliario de toda la estancia se ha pensado para aprovechar al máximo la luz y las molduras le aportan un toque elegante y distinguido. \n",
      " \n",
      " Conectado con el salón mediante amplias cristaleras encontramos la cocina, panelada en madera y totalmente equipada con los más modernos electrodomésticos. Cuenta con gran espacio de almacenamiento y una amplia encimera de mármol que se convierte en una barra ideal para desayunos o picoteos informales. \n",
      " \n",
      " Al otro lado del hall encontramos la zona de noche familiar, separada del salón comedor para garantizar la privacidad y tranquilidad. Cuenta con tres dormitorios, dos baños y un estudio distribuidos a lo largo de un pasillo. \n",
      " \n",
      " El espacio más cercano al salón es el estudio acristalado, el espacio ideal para concentrarte en tu trabajo o estudios. \n",
      " \n",
      " El dormitorio principal cuenta con un amplio armario empotrado y un completo baño en suite. Los dormitorios secundarios, uno doble y uno individual, cuentan también con armarios empotrados y comparten el segundo baño de la casa, también totalmente equipado. \n",
      " \n",
      " El acceso a la vivienda se realiza a través de un elegante portal señorial. \n",
      " \n",
      " ***La zona*** Este piso se encuentra en pleno centro de Madrid, a menos de 10 minutos andando de la Puerta del Sol, justo al lado de calles tan emblemáticas como Gran Via, Atocha o Alcalá. Además, está a un paso de los nuevos desarrollos de Canalejas, nueva meca del lujo en la ciudad. \n",
      " \n",
      " La oferta de ocio y restauración de la zona es inmensa, con actividades para todos los gustos a la vuelta de la esquina todos los días del año. Además cuenta con todos los servicios necesarios como hospitales, centros de salud y colegios. \n",
      " \n",
      " Está perfectamente conectado mediante transporte público, muy cerca de las estaciones de metro de Sol, Sevilla y Gran Vía y a menos de 100 metros de multitud de autobuses; 74, 1, 46, 146, 001, 2, N16, N21, N19, N20\n",
      " \n",
      " \n",
      " En definitiva se trata de una oportunidad única de adquirir una vivienda totalmente reformada con las máximas calidades en la zona de moda de Madrid. Ven a visitarla cuanto antes y descubramos si es la vivienda que estás buscando. \n",
      " \n",
      " —-------------------------------------------------------------------------------------------------------------------\n",
      " \n",
      " Amazing apartment in the city center fully refurbished with an a first quality tailor made decoration project. 3 bedroom, studio and two bathrooms (one bedroom with ensuite bathroom) and a great lightful living room. \n",
      " \n",
      " ***The apartment*** An elegant spacious hall welcomes you to your home acting as a buffer between Madrids buzz and your home’s peace. From the very first moment we can see the awesome living room behind the elegant siting area. \n",
      " \n",
      " Reaching the dining area, a magnificent table ideal for enjoying lovely meals with family and friends welcomes us. At the back, the living area where the great balconies to Salud street take the spotlight. Furniture in the room has been selected to maximize the effect of the light, with moldings giving an elegant distinguished feeling to the room. \n",
      " \n",
      " Connected to the loving room through wide windows we find the dark wooden kitchen, fully equipped with the latest appliances. It hay plenty of storage room and a marble counter that becomes a bar, perfect for breakfast or informal snacks. \n",
      " \n",
      " To the other side of the hall we find the night area that includes the three bedrooms, two bathrooms and the studio.\n",
      " The closest area to the living room is the glass studio, the perfect space to concentrate in your study or work without isolating from your beloved ones. \n",
      " The main bedroom has a great built in closet and a fully equipped ensuite bathroom. The secondary bedrooms, one double and one single, both have built in wardrobes and share a fully equipped bathroom located in the corridor. \n",
      " Access to the building is done through an elegant classic lobby. \n",
      " \n",
      "  ***The area*** The apartment is located right in  the center of the city, less than 10 minutes away from Puerta del Sol and just some steps away from some of the best known streets in Madrid such as Gran Vía, Alcalá or Atocha. \n",
      " \n",
      " The area has an unbeatable leisure offer that can meet anyones taste every day of the year, with Canalejas development, the new luxury area of the city right next to it. It also has all the needed services such as hospitals or schools. \n",
      " \n",
      " Public transportation is great in the area, with Sol, Gran Vía and Sevilla metro stations less than 10 minutes walking away and a wide offer of buses less than 100 meters away:  4, 1, 46, 146, 001, 2, N16, N21, N19, N20\n",
      " \n",
      " All in all it is an awesome opportunity to acquire a fully refurbished apartment with the highest qualities in a privileged area. Come visit it and lets discover if it is the home you are looking for! caracteres.\n"
     ]
    }
   ],
   "source": [
    "fila_mas_larga = df[\"descripcion_larga\"].str.len().idxmax()\n",
    "print(f\"La fila más larga es la número {fila_mas_larga} con {df.loc[fila_mas_larga, 'descripcion_larga']} caracteres.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#como tengo una version gratuita tomo una muestra de 100 filas a ver que hace\n",
    "df=df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tomo el dataframe\n",
    "#df = pd.read_csv(\"pisosCl_CharGPT.csv.csv\")\n",
    "\n",
    "# creo docs una lista de cadenas, cada una representa un piso y la disctipcion extendida\n",
    "\n",
    "docs = [f\"{url}\\n{descripcion_larga}\" for url, descripcion_larga in zip(df.url, df.descripcion_larga)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada elemento es una cadena de texto formada por la url y la  y la descripcion_extendida del anuncio del piso en venta. Utilizamos una comprensión de lista para iterar sobre las url y las descripciones de los isos en el DataFrame df.\n",
    "\n",
    "La función zip(df.url, df.description_extendida) combina los títulos y descripciones en pares.\n",
    "\n",
    "La sintaxis f\"{url}\\n{description_extendida}\" formatea cada par de url y descripcion_extendida en una cadena separada por un salto de línea \\n.\n",
    "\n",
    "Entonces, docs será una lista de cadenas, cada una representando un artículo con su título y su descripción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, initialize the OpenAI client and generate the embeddings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "client = OpenAI(\n",
    "  organization='org-29axgWAg48fPvMEA0VyVUUMe',\n",
    "  project='$PROJECT_PRICE_HOUSES',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#client = OpenAI('org-29axgWAg48fPvMEA0VyVUUMe')\u001b[39;00m\n\u001b[0;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msk-proj-sOF90bMrJZA2VVRoXIeO3-WW6Hu7JrfKkWvoa_YCrmXNEIVp-_B5V3EAdiYapxwac6JfM3zjTCT3BlbkFJPNjGv9e1G2WF8DDhvoT6XV3NM13Ig8Cm5NvTGUyK8yMdTfWP41JEqRGIAYXmxTFhP7ytpgUgYA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-3-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(x\u001b[38;5;241m.\u001b[39membedding) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata]\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\openai\\resources\\embeddings.py:128\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    122\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    123\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\openai\\_base_client.py:1296\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1284\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1292\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1293\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1294\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1295\u001b[0m     )\n\u001b[1;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\openai\\_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    971\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\openai\\_base_client.py:1062\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1061\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\openai\\_base_client.py:1111\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\openai\\_base_client.py:1062\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1061\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\openai\\_base_client.py:1111\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\extas\\anaconda3\\envs\\Nuclio\\Lib\\site-packages\\openai\\_base_client.py:1077\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1076\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1077\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1080\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1081\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1086\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "#client = OpenAI('org-29axgWAg48fPvMEA0VyVUUMe')\n",
    "client = OpenAI(api_key='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')\n",
    "response = client.embeddings.create(input=docs, model=\"text-embedding-3-small\")\n",
    "embeddings = [np.array(x.embedding) for x in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster documents\n",
    "Once you have the embeddings, you can cluster them using hdbscan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=3).fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will generate clusters using the embeddings generated, and then create a DataFrame with the results. Itfits the hdbscan algorithm. In this case, I set min_samples and min_cluster_size to 3, but depending on your data this may change. Check HDBSCAN’s documentation to learn more about these parameters.\n",
    "\n",
    "Next, you’ll create topic titles for each cluster based on their contents.\n",
    "\n",
    "Visualize the clusters\n",
    "After you’ve generated the clusters, you can visualize them using UMAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = UMAP(n_components=2, random_state=42, n_neighbors=80, min_dist=0.1)\n",
    "\n",
    "df_umap = (\n",
    "    pd.DataFrame(umap.fit_transform(np.array(embeddings)), columns=['x', 'y'])\n",
    "    .assign(cluster=lambda df: hdb.labels_.astype(str))\n",
    "    .query('cluster != \"-1\"')\n",
    "    .sort_values(by='cluster')\n",
    ")\n",
    "\n",
    "fig = px.scatter(df_umap, x='x', y='y', color='cluster')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Topic Title per Cluster\n",
    "For each cluster, you’ll generate a topic title summarizing the articles in that cluster. Copy the following code to your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster_name\"] = \"Uncategorized\"\n",
    "\n",
    "def generate_topic_titles():\n",
    "    system_message = \"You're an expert journalist. You're helping me write short but compelling topic titles for groups of news articles.\"\n",
    "    user_template = \"Using the following articles, write a 4 to 5 word title that summarizes them.\\n\\nARTICLES:\\n\\n{}\\n\\nTOPIC TITLE:\"\n",
    "\n",
    "    for c in df.cluster.unique():\n",
    "        sample_articles = df.query(f\"cluster == '{c}'\").to_dict(orient=\"records\")\n",
    "        articles_str = \"\\n\\n\".join(\n",
    "            [\n",
    "                f\"[{i}] {article['title']}\\n{article['description'][:200]}{'...' if len(article['description']) > 200 else ''}\"\n",
    "                for i, article in enumerate(\n",
    "                    sample_articles, start=1\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_template.format(articles_str)},\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages, temperature=0.7, seed=42\n",
    "        )\n",
    "\n",
    "        topic_title = response.choices[0].message.content\n",
    "        df.loc[df.cluster == c, \"cluster_name\"] = topic_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes all the articles per cluster and uses gpt-3.5-turbo to generate a relevant topic title from them. Itgoes through each cluster, takes the articles in it, and makes a prompt using that to generate a topic title for that cluster.\n",
    "\n",
    "Finally, you can check the resulting clusters and topic titles, as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 6\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    print(df.query(f\"cluster == '{c}'\").topic_title.values[0])\n",
    "    display(df.query(f\"cluster == '{c}'\").drop(columns=[\"topic_title\"]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, running this code produces the following articles and topic titles:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nuclio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
