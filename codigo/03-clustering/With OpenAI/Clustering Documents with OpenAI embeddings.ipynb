{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlace al gib de ejmplo : https://github.com/dylanjcastillo/random/tree/main/self-organizing-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering Documents\n",
    "You should think of the clustering process in three steps:\n",
    "\n",
    "Generate numerical vector representations of documents using OpenAI’s embedding capabilities.\n",
    "Apply a clustering algorithm on the vectors to group the documents.\n",
    "Generate a title for each cluster summarizing the articles contained in it.\n",
    "That’s it! Now, you’ll see how that looks in practice.\n",
    "\n",
    "Import the Required Packages\n",
    "Start by importing the required Python libraries. Copy the following code in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from umap import UMAP\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tomo el dataframe\n",
    "df = pd.read_csv(\"news_data_dedup.csv\")\n",
    "\n",
    "# creo docs una lista de cadenas, cada una representa un piso y la disctipcion extendida\n",
    "\n",
    "docs = [f\"{url}\\n{description_extendida}\" for url, description_extendida in zip(df.url, df.description_extendida)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada elemento es una cadena de texto formada por la url y la  y la descripcion_extendida del anuncio del piso en venta. Utilizamos una comprensión de lista para iterar sobre las url y las descripciones de los isos en el DataFrame df.\n",
    "\n",
    "La función zip(df.url, df.description_extendida) combina los títulos y descripciones en pares.\n",
    "\n",
    "La sintaxis f\"{url}\\n{description_extendida}\" formatea cada par de url y descripcion_extendida en una cadena separada por un salto de línea \\n.\n",
    "\n",
    "Entonces, docs será una lista de cadenas, cada una representando un artículo con su título y su descripción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, initialize the OpenAI client and generate the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.embeddings.create(input=docs, model=\"text-embedding-3-small\")\n",
    "embeddings = [np.array(x.embedding) for x in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster documents\n",
    "Once you have the embeddings, you can cluster them using hdbscan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=3).fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will generate clusters using the embeddings generated, and then create a DataFrame with the results. Itfits the hdbscan algorithm. In this case, I set min_samples and min_cluster_size to 3, but depending on your data this may change. Check HDBSCAN’s documentation to learn more about these parameters.\n",
    "\n",
    "Next, you’ll create topic titles for each cluster based on their contents.\n",
    "\n",
    "Visualize the clusters\n",
    "After you’ve generated the clusters, you can visualize them using UMAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = UMAP(n_components=2, random_state=42, n_neighbors=80, min_dist=0.1)\n",
    "\n",
    "df_umap = (\n",
    "    pd.DataFrame(umap.fit_transform(np.array(embeddings)), columns=['x', 'y'])\n",
    "    .assign(cluster=lambda df: hdb.labels_.astype(str))\n",
    "    .query('cluster != \"-1\"')\n",
    "    .sort_values(by='cluster')\n",
    ")\n",
    "\n",
    "fig = px.scatter(df_umap, x='x', y='y', color='cluster')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Topic Title per Cluster\n",
    "For each cluster, you’ll generate a topic title summarizing the articles in that cluster. Copy the following code to your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster_name\"] = \"Uncategorized\"\n",
    "\n",
    "def generate_topic_titles():\n",
    "    system_message = \"You're an expert journalist. You're helping me write short but compelling topic titles for groups of news articles.\"\n",
    "    user_template = \"Using the following articles, write a 4 to 5 word title that summarizes them.\\n\\nARTICLES:\\n\\n{}\\n\\nTOPIC TITLE:\"\n",
    "\n",
    "    for c in df.cluster.unique():\n",
    "        sample_articles = df.query(f\"cluster == '{c}'\").to_dict(orient=\"records\")\n",
    "        articles_str = \"\\n\\n\".join(\n",
    "            [\n",
    "                f\"[{i}] {article['title']}\\n{article['description'][:200]}{'...' if len(article['description']) > 200 else ''}\"\n",
    "                for i, article in enumerate(\n",
    "                    sample_articles, start=1\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_template.format(articles_str)},\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages, temperature=0.7, seed=42\n",
    "        )\n",
    "\n",
    "        topic_title = response.choices[0].message.content\n",
    "        df.loc[df.cluster == c, \"cluster_name\"] = topic_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes all the articles per cluster and uses gpt-3.5-turbo to generate a relevant topic title from them. Itgoes through each cluster, takes the articles in it, and makes a prompt using that to generate a topic title for that cluster.\n",
    "\n",
    "Finally, you can check the resulting clusters and topic titles, as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 6\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    print(df.query(f\"cluster == '{c}'\").topic_title.values[0])\n",
    "    display(df.query(f\"cluster == '{c}'\").drop(columns=[\"topic_title\"]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, running this code produces the following articles and topic titles:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
